#!/usr/bin/env python
"""
This script divides the list of USAR experiments in a .csv dataset such that they can be processed
in parallel. Since the parameters are not shared across experiments, we can perform inferences in
multiple models in parallel if we have computing resources.

Using the multiprocessing library to spawn an inference function in parallel in a single script
results in a 'AssertionError: daemonic processes are not allowed to have children'. This is due to
the multiple processes creates during inference (one per chain) and the impossibility of creating
children processes from processes spawn with multiprocessing library. Therefore, we do this
parallelization via shell commands. We split the experiments into different processes which are
performed in different TMUX windows of the same TMUX session. If the tmux session does not exist,
this script will create one named as the run ID. If the number of experiments is bigger than the
number of processes they should be split into, some processes will be responsible for performing
inference sequentially in the experiments assigned to them.

Note:
1. This script requires TMUX to be installed in the machine.
2. This script requires Conda to be installed in the machine and have an environment called
    'coordination' with the dependencies listed in requirements.txt installed.
3. Before running this script for the first time, the user has to check if tmux initializes
    conda when a new session or window is created. This is accomplished by copying the conda
    initialization script, typically saved in the shell rc file (e.g., .bashrc), to their shell
    profile file (e.g., .bash_profile).
"""

import argparse
import json
import multiprocessing
import os
from datetime import datetime
from typing import Optional

import numpy as np
import pandas as pd

from coordination.common.constants import (DEFAULT_BURN_IN, DEFAULT_NUM_CHAINS,
                                           DEFAULT_NUM_JOBS,
                                           DEFAULT_NUM_SAMPLES,
                                           DEFAULT_NUTS_INIT_METHOD,
                                           DEFAULT_PROGRESS_SAVING_FREQUENCY,
                                           DEFAULT_SEED, DEFAULT_TARGET_ACCEPT)
from coordination.common.tmux import TMUX
from coordination.common.utils import NumpyArrayEncoder
from coordination.model.builder import ModelBuilder
from coordination.model.config.mapper import DataMapper


def infer(out_dir: str,
          evidence_filepath: str,
          model_params_dict_filepath: Optional[str],
          data_mapping_filepath: str,
          model_name: str,
          do_prior: bool,
          do_posterior: bool,
          seed: Optional[int],
          burn_in: int,
          num_samples: int,
          num_chains: int,
          num_jobs_per_inference: int,
          nuts_init_method: str,
          target_accept: float,
          num_inference_jobs: int,
          progress_saving_frequency: int):
    """
    Runs inference for multiple experiments in parallel.

    @param out_dir: directory where results must be saved.
    @param evidence_filepath: path of the .csv file containing evidence for the model.
    @param model_params_dict_filepath: path of the .json file containing values to replace model
        parameter's default values.
    @param data_mapping_filepath: path to the file containing a mapping between config bundle
        attributes and columns in the evidence dataframe.
    @param model_name: name of the model. One of ["vocalic, "vocalic_semantic"].
    @param do_prior: whether to perform prior predictive checks.
    @param do_posterior: whether to fit the model and perform posterior predictive checks.
    @param seed: random seed for reproducibility.
    @param burn_in: number of samples in the warm-up phase.
    @param num_samples: number of samples from the posterior distribution.
    @param num_chains: number of parallel chains.
    @param num_jobs_per_inference: number of jobs per inference up to the number of chains.
    @param nuts_init_method: initialization method for the NUTS inference algorithm.
    @param target_accept: acceptance probability.
    @param num_inference_jobs: number of jobs to split the different experiments in the data into.
        Each one of these jobs will spawn min(num_chain, num_jobs_per_inference) other processes.
    @param progress_saving_frequency: frequency to save progress in number of samples drawn per
        chain.
    """

    # Parameters passed to this function relevant for post-analysis. We will save them to a file.
    execution_params = locals().copy()
    del execution_params["out_dir"]
    del execution_params["model_params_dict_filepath"]
    del execution_params["data_mapping_filepath"]

    # Leave 20% of the cores free
    num_cores = int(multiprocessing.cpu_count() * 0.8)
    num_required_cores = min(num_chains, num_jobs_per_inference) * num_inference_jobs
    if multiprocessing.cpu_count() < min(num_chains, num_jobs_per_inference) * num_inference_jobs:
        raise ValueError(f"The machine has {num_cores} cores but {num_required_cores} are "
                         f"required for full parallelization. Reduce the number of inference "
                         f"jobs or the number of jobs per inference and try again.")

    if not do_prior and not do_posterior:
        raise ValueError("No inference to be performed. Set do_prior and/or do_posterior to True.")

    if not os.path.exists(evidence_filepath):
        raise FileNotFoundError(f"Evidence not found in {evidence_filepath}.")

    model_params_dict = None
    if model_params_dict_filepath:
        if not os.path.exists(model_params_dict_filepath):
            raise FileNotFoundError(
                f"Dictionary of parameter values not found in {model_params_dict_filepath}.")

        with open(model_params_dict_filepath) as f:
            model_params_dict = json.load(f)

    if not os.path.exists(data_mapping_filepath):
        raise FileNotFoundError(f"Mapping between evidence and model's parameter not found in "
                                f"{data_mapping_filepath}.")

    run_id = datetime.now().strftime("%Y.%m.%d--%H.%M.%S")
    execution_params["run_id"] = run_id

    evidence_df = pd.read_csv(evidence_filepath, index_col=0)
    config_bundle = ModelBuilder.build_bundle(model_name)
    with open(data_mapping_filepath) as f:
        data_mapping_dict = json.load(f)
        data_mapper = DataMapper(data_mapping_dict)

    config_bundle.update(model_params_dict)
    data_mapper.validate(config_bundle, evidence_df.columns)

    # Update execution params with model config bundle and data mapping.
    execution_params["model_config_bundle"] = config_bundle.__dict__
    execution_params["data_mapper"] = data_mapping_dict

    results_folder = f"{out_dir}/{run_id}"
    os.makedirs(results_folder, exist_ok=True)

    print(f"\nInferences will be saved in {results_folder}.")

    # Save execution parameters
    with open(f"{results_folder}/execution_params.json", "w") as f:
        json.dump(execution_params, f, indent=4, cls=NumpyArrayEncoder)

    # Get absolute path to the bin directory in the project's root folder. We will use it to
    # execute run_inference_sequentially.py from a tmux window.
    bin_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "."))

    experiments = sorted(list(evidence_df["experiment_id"].unique()))
    experiment_blocks = np.array_split(
        experiments, min(num_inference_jobs, len(experiments))
    )
    tmux = TMUX(run_id.replace(".", "_").replace("-", "_"))
    for i, experiments_per_process in enumerate(experiment_blocks):
        # Start a new process in a tmux window for the experiments to be processed.
        experiment_ids = ",".join(experiments_per_process)
        tmux_window_name = experiments_per_process[0]
        if len(experiments_per_process) > 1:
            tmux_window_name += "..."

        # Call the actual inference script (bin/run_inference_sequentially)
        call_python_script_command = (
            f'python3 {bin_dir}/run_inference_sequentially '
            f'--out_dir="{results_folder}" '
            f'--experiment_ids="{experiment_ids}" '
            f'--evidence_filepath="{evidence_filepath}" '
            f'--model_params_dict_filepath="{model_params_dict_filepath}" '
            f'--data_mapping_filepath="{data_mapping_filepath}" '
            f'--model_name="{model_name}" '
            f'--do_prior={do_prior} '
            f'--do_posterior={do_posterior} '
            f'--seed={seed} '
            f'--burn_in={burn_in} '
            f'--num_samples={num_samples} '
            f'--num_jobs={num_jobs_per_inference} '
            f'--nuts_init_method="{nuts_init_method}" '
            f'--target_accept={target_accept} '
            f'--progress_saving_frequency={progress_saving_frequency}'
        )

        print(f"\nCalling the following command in the TMUX window {tmux_window_name}.")
        print(call_python_script_command)

        tmux.create_window(tmux_window_name)
        tmux.run_command("conda activate coordination")
        tmux.run_command(call_python_script_command)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Runs NUTS inference algorithm for multiple experiments in a .csv file in "
                    "parallel. This script calls the bin/run_inference_sequentially multiple "
                    "times with a series of experiments to be executed in sequence in each job. "
                    ""
                    "This script has an associated run_id (a timestamp) which names a folder that "
                    "is created under out_dir to store results and relevant data. A new TMUX "
                    "session is created named as the run_id and each inference process will run "
                    "in a separate tmux window of that session."
                    ""
                    "Each experiment will spawn min(num_jobs, num_chains) jobs. Thus, the machine "
                    "needs num_inference_jobs * min(num_jobs, num_chains) available cores to "
                    "achieve full parallelization."
    )

    parser.add_argument(
        "--out_dir",
        type=str,
        default=os.getenv("OUT_DIR", ".run"),
        required=False,
        help="Directory where artifacts must be saved.",
    )
    parser.add_argument(
        "--evidence_filepath",
        type=str,
        required=True,
        help="Path of the .csv file containing the evidence data.",
    )
    parser.add_argument(
        "--model_name",
        type=str,
        required=True,
        choices=["vocalic", "vocalic_semantic"],
        help="Model name.",
    )
    parser.add_argument(
        "--model_params_dict_filepath",
        type=str,
        required=False,
        help="Path of the .json file containing parameters of the model to override default "
             "values."
    )
    parser.add_argument(
        "--data_mapping_filepath",
        type=str,
        required=True,
        help="Path of the .json file containing a mapping between column names in the .csv file "
             "containing evidence to the model and config bundle parameters required for "
             "inference."
    )
    parser.add_argument(
        "--do_prior",
        type=int,
        required=False,
        default=1,
        help="Whether to perform prior predictive check. Use the value 0 to deactivate.",
    )
    parser.add_argument(
        "--do_posterior",
        type=int,
        required=False,
        default=1,
        help="Whether to perform posterior inference. Use the value 0 to deactivate.",
    )
    parser.add_argument(
        "--seed",
        type=int,
        required=False,
        default=DEFAULT_SEED,
        help="Random seed to use during inference.",
    )
    parser.add_argument(
        "--burn_in",
        type=int,
        required=False,
        default=DEFAULT_BURN_IN,
        help="Number of samples to discard per chain during posterior inference.",
    )
    parser.add_argument(
        "--num_samples",
        type=int,
        required=False,
        default=DEFAULT_NUM_SAMPLES,
        help="Number of samples to keep per chain during posterior inference.",
    )
    parser.add_argument(
        "--num_chains",
        type=int,
        required=False,
        default=DEFAULT_NUM_CHAINS,
        help="Number of chains to use during posterior inference.",
    )
    parser.add_argument(
        "--num_jobs_per_inference",
        type=int,
        required=False,
        default=DEFAULT_NUM_JOBS,
        help="Number of jobs to use per inference process. The effective number of jobs is "
             "min(num_jobs, num_chains).",
    )
    parser.add_argument(
        "--nuts_init_method",
        type=str,
        required=False,
        default=DEFAULT_NUTS_INIT_METHOD,
        help="NUTS initialization method.",
    )
    parser.add_argument(
        "--target_accept",
        type=float,
        required=False,
        default=DEFAULT_TARGET_ACCEPT,
        help="Target acceptance probability used to control step size and reduce "
             "divergences during inference.",
    )
    parser.add_argument(
        "--num_inference_jobs",
        type=int,
        required=False,
        default=os.getenv("NUM_JOBS", DEFAULT_NUM_JOBS),
        help="Number of jobs to split the experiments into.",
    )
    parser.add_argument(
        "--progress_saving_frequency",
        type=int,
        required=False,
        default=DEFAULT_PROGRESS_SAVING_FREQUENCY,
        help="Frequency at which to save the progress in number of samples. For instance, if x, "
             "progress will be saved at every x samples per chain.",
    )

    args = parser.parse_args()

    infer(out_dir=args.out_dir,
          evidence_filepath=args.evidence_filepath,
          model_params_dict_filepath=args.model_params_dict_filepath,
          data_mapping_filepath=args.data_mapping_filepath,
          model_name=args.model_name,
          do_prior=args.do_prior,
          do_posterior=args.do_posterior,
          seed=args.seed,
          burn_in=args.burn_in,
          num_samples=args.num_samples,
          num_chains=args.num_chains,
          num_jobs_per_inference=args.num_jobs_per_inference,
          nuts_init_method=args.nuts_init_method,
          target_accept=args.target_accept,
          num_inference_jobs=args.num_inference_jobs,
          progress_saving_frequency=args.progress_saving_frequency)
