#!/usr/bin/env python
"""
This script performs inferences in a subset of USAR experiments from a .csv file with vocalic and
semantic link data. Inferences are performed sequentially, i.e., experiment by experiment until
all experiments are covered.
"""

import argparse
import json
import logging
import os
import sys
from typing import List, Optional

import pandas as pd

from coordination.callback.progress_saver_callback import ProgressSaverCallback
from coordination.common.constants import (DEFAULT_BURN_IN,
                                           DEFAULT_INFERENCE_RESULTS_DIR,
                                           DEFAULT_NUM_CHAINS,
                                           DEFAULT_NUM_JOBS_PER_INFERENCE,
                                           DEFAULT_NUM_SAMPLES,
                                           DEFAULT_NUTS_INIT_METHOD,
                                           DEFAULT_PROGRESS_SAVING_FREQUENCY,
                                           DEFAULT_SEED, DEFAULT_TARGET_ACCEPT)
from coordination.common.log import configure_log
from coordination.model.builder import ModelBuilder, MODELS
from coordination.model.config_bundle.mapper import DataMapper

# PyMC 5.0.2 prints some warnings when we use GaussianRandomWalk. The snippet below silences them.
if not sys.warnoptions:
    import warnings

    warnings.simplefilter("ignore")


def infer(
        out_dir: str,
        experiment_ids: List[str],
        evidence_filepath: str,
        model_params_dict_filepath: Optional[str],
        data_mapping_filepath: str,
        model_name: str,
        do_prior: bool,
        do_posterior: bool,
        seed: Optional[int],
        burn_in: int,
        num_samples: int,
        num_chains: int,
        num_jobs: int,
        nuts_init_method: str,
        target_accept: float,
        progress_saving_frequency: int):
    """
    Runs inference for multiple experiments in sequence.

    @param out_dir: directory where results must be saved.
    @param experiment_ids: a comma-separated list of experiment ids to run inference for.
    @param evidence_filepath: path of the .csv file containing evidence for the model.
    @param model_params_dict_filepath: path of the .json file containing values to replace model
        parameter's default values.
    @param data_mapping_filepath: path to the file containing a mapping between config bundle
        attributes and columns in the evidence dataframe.
    @param model_name: name of the model. One of ["vocalic, "vocalic_semantic"].
    @param do_prior: whether to perform prior predictive checks.
    @param do_posterior: whether to fit the model and perform posterior predictive checks.
    @param seed: random seed for reproducibility.
    @param burn_in: number of samples in the warm-up phase.
    @param num_samples: number of samples from the posterior distribution.
    @param num_chains: number of parallel chains.
    @param num_jobs: number of jobs per inference up to the number of chains.
    @param nuts_init_method: initialization method for the NUTS inference algorithm.
    @param target_accept: acceptance probability.
    @param progress_saving_frequency: frequency to save progress in number of samples drawn per
        chain.
    """

    if not do_prior and not do_posterior:
        raise ValueError("No inference to be performed. Set do_prior and/or do_posterior to True.")

    if not os.path.exists(evidence_filepath):
        raise FileNotFoundError(f"Evidence not found in {evidence_filepath}.")

    model_params_dict = None
    if model_params_dict_filepath:
        if not os.path.exists(model_params_dict_filepath):
            raise FileNotFoundError(
                f"Dictionary of parameter values not found in {model_params_dict_filepath}.")

        with open(model_params_dict_filepath) as f:
            model_params_dict = json.load(f)

    if not os.path.exists(data_mapping_filepath):
        raise FileNotFoundError(f"Mapping between evidence and model's parameter not found in "
                                f"{data_mapping_filepath}.")

    with open(data_mapping_filepath) as f:
        data_mapping_dict = json.load(f)
        data_mapper = DataMapper(data_mapping_dict)

    evidence_df = pd.read_csv(evidence_filepath, index_col=0)
    evidence_df = evidence_df[evidence_df["experiment_id"].isin(experiment_ids)]

    experiments_in_evidence = sorted(evidence_df["experiment_id"].tolist())
    experiments_not_in_evidence = sorted(
        list(set(experiment_ids).difference(set(experiments_in_evidence))))

    experiment_list = "\n".join(experiments_in_evidence)
    print(f"\nRunning inference for experiments:\n{experiment_list}\n")

    if len(experiments_not_in_evidence) > 0:
        experiment_list = "\n".join(experiments_not_in_evidence)
        print(f"\nSkipping inference for experiments not in the evidence file: {experiment_list}")

    for experiment_id in experiment_ids:
        row_df = evidence_df[evidence_df["experiment_id"] == experiment_id].iloc[0, :]
        experiment_out_dir = f"{out_dir}/{experiment_id}"
        os.makedirs(experiment_out_dir, exist_ok=True)

        # All the logs from now on will be written to a file inside the folder with the results for
        # the experiment.
        configure_log(verbose=True, log_filepath=f"{experiment_out_dir}/log.txt")
        logger = logging.getLogger()
        logger.setLevel(logging.INFO)

        logger.info(f"\nProcessing {experiment_id}...")

        # # Data transformation to correct biological differences captured in the signals from
        # different participants. evidence.normalize_per_subject()
        try:
            config_bundle = ModelBuilder.build_bundle(model_name)
            config_bundle.update(model_params_dict)
            data_mapper.update_config_bundle(config_bundle, row_df)
            model = ModelBuilder.build_model(model_name=model_name, config_bundle=config_bundle)
            model.prepare_for_inference()

            idata = None
            if do_prior:
                logger.info("Prior predictive check")
                try:
                    idata = model.prior_predictive(num_samples=num_samples, seed=seed)
                except Exception as ex:
                    logger.error(ex)

            if do_posterior:
                logger.info("Fitting model...")
                try:
                    idata_posterior = model.fit(
                        seed=seed,
                        burn_in=burn_in,
                        num_samples=num_samples,
                        num_chains=num_chains,
                        num_jobs=num_jobs,
                        nuts_init_method=nuts_init_method,
                        target_accept=target_accept,
                        callback=ProgressSaverCallback(out_dir=experiment_out_dir,
                                                       saving_frequency=progress_saving_frequency))
                except Exception as ex:
                    logger.error(ex)
                    return

                if idata is None:
                    idata = idata_posterior
                else:
                    idata.add(idata_posterior)

                idata_posterior_predictive = model.posterior_predictive(posterior_trace=idata.trace,
                                                                        seed=seed)
                idata.add(idata_posterior_predictive)

                # Log the percentage of divergences
                perc_divergences = 100.0 * idata.num_divergences / idata.num_posterior_samples
                logger.info(
                    f"{idata.num_divergences} divergences in {idata.num_posterior_samples} samples "
                    f"--> {perc_divergences}%."
                )

            idata.save_to_directory(experiment_out_dir)
            logger.info("[SUCCESS]")
        except Exception as ex:
            logger.error(ex)
            raise ex


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Runs NUTS inference algorithm per experiment (in sequence) for a series of "
                    "experiments in a .csv file. Prior predictive checks can be performed with or "
                    "without posterior inference and checks. A final inference data object with "
                    "the results is saved by experiment in its corresponding folder created under "
                    "the informed output directory. Individual logs are also saved in a txt file "
                    "under the same folder."
    )
    parser.add_argument(
        "--out_dir",
        type=str,
        default=os.getenv("INFERENCE_RESULTS_DIR", DEFAULT_INFERENCE_RESULTS_DIR),
        required=False,
        help="Directory where artifacts must be saved.",
    )
    parser.add_argument(
        "--experiment_ids",
        type=str,
        required=True,
        help="A list of experiment ids separated by comma for which we want to perform inference. "
             "If more than one experiment is provided, inference will be performed sequentially, "
             "i.e., for one experiment at a time. Experiment ids must be separated by comma.",
    )
    parser.add_argument(
        "--evidence_filepath",
        type=str,
        required=True,
        help="Path of the .csv file containing the evidence data.",
    )
    parser.add_argument(
        "--model_name",
        type=str,
        required=True,
        choices=MODELS,
        help="Model name.",
    )
    parser.add_argument(
        "--model_params_dict_filepath",
        type=str,
        required=False,
        help="Path of the .json file containing parameters of the model to override default "
             "values."
    )
    parser.add_argument(
        "--data_mapping_filepath",
        type=str,
        required=True,
        help="Path of the .json file containing a mapping between column names in the .csv file "
             "containing evidence to the model and config bundle parameters required for "
             "inference."
    )
    parser.add_argument(
        "--do_prior",
        type=int,
        required=False,
        default=1,
        help="Whether to perform prior predictive check. Use the value 0 to deactivate.",
    )
    parser.add_argument(
        "--do_posterior",
        type=int,
        required=False,
        default=1,
        help="Whether to perform posterior inference. Use the value 0 to deactivate.",
    )
    parser.add_argument(
        "--seed",
        type=int,
        required=False,
        default=DEFAULT_SEED,
        help="Random seed to use during inference.",
    )
    parser.add_argument(
        "--burn_in",
        type=int,
        required=False,
        default=DEFAULT_BURN_IN,
        help="Number of samples to discard per chain during posterior inference.",
    )
    parser.add_argument(
        "--num_samples",
        type=int,
        required=False,
        default=DEFAULT_NUM_SAMPLES,
        help="Number of samples to keep per chain during posterior inference.",
    )
    parser.add_argument(
        "--num_chains",
        type=int,
        required=False,
        default=DEFAULT_NUM_CHAINS,
        help="Number of chains to use during posterior inference.",
    )
    parser.add_argument(
        "--num_jobs",
        type=int,
        required=False,
        default=DEFAULT_NUM_JOBS_PER_INFERENCE,
        help="Number of jobs to use per inference process. The effective number of jobs is "
             "min(num_jobs, num_chains).",
    )
    parser.add_argument(
        "--nuts_init_method",
        type=str,
        required=False,
        default=DEFAULT_NUTS_INIT_METHOD,
        help="NUTS initialization method.",
    )
    parser.add_argument(
        "--target_accept",
        type=float,
        required=False,
        default=DEFAULT_TARGET_ACCEPT,
        help="Target acceptance probability used to control step size and reduce "
             "divergences during inference.",
    )
    parser.add_argument(
        "--progress_saving_frequency",
        type=int,
        required=False,
        default=DEFAULT_PROGRESS_SAVING_FREQUENCY,
        help="Frequency at which to save the progress in number of samples. For instance, if x, "
             "progress will be saved at every x samples per chain.",
    )

    args = parser.parse_args()

    infer(out_dir=args.out_dir,
          experiment_ids=args.experiment_ids.split(","),
          evidence_filepath=args.evidence_filepath,
          model_params_dict_filepath=args.model_params_dict_filepath,
          data_mapping_filepath=args.data_mapping_filepath,
          model_name=args.model_name,
          do_prior=args.do_prior,
          do_posterior=args.do_posterior,
          seed=args.seed,
          burn_in=args.burn_in,
          num_samples=args.num_samples,
          num_chains=args.num_chains,
          num_jobs=args.num_jobs,
          nuts_init_method=args.nuts_init_method,
          target_accept=args.target_accept,
          progress_saving_frequency=args.progress_saving_frequency)
