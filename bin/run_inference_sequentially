#!/usr/bin/env python

import argparse
import json
import logging
import os
import pickle
import sys
from typing import Any, List, Optional

import arviz as az
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from tqdm import tqdm

from coordination.callback.progress_saver_callback import ProgressSaverCallback

from coordination.common.constants import (DEFAULT_BURN_IN, DEFAULT_NUM_CHAINS,
                                           DEFAULT_NUM_JOBS,
                                           DEFAULT_NUM_SAMPLES,
                                           DEFAULT_NUM_SUBJECTS,
                                           DEFAULT_NUTS_INIT_METHOD,
                                           DEFAULT_SEED, DEFAULT_TARGET_ACCEPT)
from coordination.common.log import configure_log
from coordination.common.types import ParameterValueType
from coordination.common.utils import set_random_seed
from coordination.model.builder import ModelBuilder
from coordination.model.config.mapper import DataMapper
from coordination.model.coordination_model import CoordinationPosteriorSamples
from coordination.model.vocalic_model import (VOCALIC_FEATURES, VocalicModel,
                                              VocalicSeries)
from coordination.model.vocalic_semantic_model import (VocalicSemanticModel,
                                                       VocalicSemanticSeries)

"""
This script performs inferences in a subset of USAR experiments from a .csv file with vocalic and 
semantic link data. Inferences are performed sequentially, i.e., experiment by experiment until 
all experiments are covered. 
"""

# PyMC 5.0.2 prints some warnings when we use GaussianRandomWalk. The snippet below silences them.
if not sys.warnoptions:
    import warnings

    warnings.simplefilter("ignore")


def infer(
        out_dir: str,
        experiment_ids: List[str],
        evidence_filepath: str,
        model_params_dict_filepath: Optional[str],
        data_mapping_filepath: str,
        model_name: str,
        do_prior: bool,
        do_posterior: bool,
        seed: Optional[int],
        burn_in: int,
        num_samples: int,
        num_chains: int,
        num_jobs: int,
        nuts_init_method: str,
        target_accept: float):
    logger = logging.getLogger()

    if not do_prior and not do_posterior:
        logger.error(
            "No inference to be performed. Set do_prior and/or do_posterior to True."
        )
        return

    model_params_dict = None
    if model_params_dict_filepath:
        if not os.path.exists(model_params_dict_filepath):
            logger.error(
                f"Dictionary of parameter values not found in {model_params_dict_filepath}."
            )
            return

        with open(model_params_dict_filepath) as f:
            model_params_dict = json.load(f)

    if not os.path.exists(data_mapping_filepath):
        logger.error(
            f"Mapping between evidence and model's parameter not found in "
            f"{data_mapping_filepath}."
        )
        return

    with open(data_mapping_filepath) as f:
        data_mapping_dict = json.load(f)
        data_mapper = DataMapper(data_mapping_dict)

    evidence_df = pd.read_csv(evidence_filepath, index_col=0)
    evidence_df = evidence_df[evidence_df["experiment_id"].isin(experiment_ids)]

    experiments_in_evidence = sorted(evidence_df["experiment_id"].tolist())
    experiments_not_in_evidence = sorted(
        list(set(experiment_ids).difference(set(experiments_in_evidence))))

    experiment_list = "\n".join(experiments_in_evidence)
    logger.info(f"\nRunning inference for experiments: {experiment_list}")

    if len(experiments_not_in_evidence) > 0:
        experiment_list = "\n".join(experiments_not_in_evidence)
        logger.info(
            f"\nSkipping inference for experiments not in the evidence file: {experiment_list}")

    for _, row_df in evidence_df.iterrows():
        experiment_id = row_df["experiment_id"]
        experiment_out_dir = f"{out_dir}/{experiment_id}"
        os.makedirs(experiment_out_dir, exist_ok=True)

        # All the logs from now on will be written to a file inside the folder with the results for
        # the experiment.
        configure_log(verbose=True, log_filepath=f"{experiment_out_dir}/log.txt")
        logger = logging.getLogger()
        logger.setLevel(logging.INFO)

        logger.info(f"\nProcessing {experiment_id}...")

        # # Data transformation to correct biological differences captured in the signals from
        # different participants. evidence.normalize_per_subject()

        config_bundle = ModelBuilder.build_bundle(model_name)
        config_bundle.update(model_params_dict)
        data_mapper.update_config_bundle(config_bundle, row_df)
        model = ModelBuilder.build_model(model_name=model_name, config_bundle=config_bundle)
        model.create_random_variables()

        idata = None
        if do_prior:
            logger.info("Prior predictive check")
            idata = model.prior_predictive(num_samples=num_samples, seed=seed)

        if do_posterior:
            logger.info("Fitting model...")
            idata_posterior = model.fit(
                seed=seed,
                burn_in=burn_in,
                num_samples=num_samples,
                num_chains=num_chains,
                num_jobs=num_jobs,
                nuts_init_method=nuts_init_method,
                target_accept=target_accept,
                callback=ProgressSaverCallback(out_dir=experiment_out_dir, saving_frequency=10))

            if idata is None:
                idata = idata_posterior
            else:
                idata.add(idata_posterior)

            idata_posterior_predictive = model.posterior_predictive(posterior_trace=idata.trace,
                                                                    seed=seed)
            idata.add(idata_posterior_predictive)

            # Log the percentage of divergences
            perc_divergences = 100.0 * idata.num_divergences / idata.num_posterior_samples
            logger.info(
                f"{idata.num_divergences} divergences in {idata.num_posterior_samples} samples "
                f"--> {perc_divergences}%."
            )

        idata.save(f"{experiment_out_dir}/inference_data")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Runs NUTS inference algorithm per experiment (in sequence) for a series of "
                    "experiments in a .csv file. Prior predictive checks can be performed with or "
                    "without posterior inference and checks. A final inference data object with "
                    "the results is saved by experiment in its corresponding folder created under "
                    "the informed output directory. Individual logs are also saved in a txt file "
                    "under the same folder."
    )
    parser.add_argument(
        "--out_dir",
        type=str,
        required=True,
        help="Directory where artifacts must be saved.",
    )
    parser.add_argument(
        "--experiment_ids",
        type=str,
        required=True,
        help="A list of experiment ids separated by comma for which we want to perform inference. "
             "If more than one experiment is provided, inference will be performed sequentially, "
             "i.e., for one experiment at a time. Experiment ids must be separated by comma.",
    )
    parser.add_argument(
        "--evidence_filepath",
        type=str,
        required=True,
        help="Path of the .csv file containing the evidence data.",
    )
    parser.add_argument(
        "--model_name",
        type=str,
        required=True,
        choices=["vocalic", "vocalic_semantic"],
        help="Model name.",
    )
    parser.add_argument(
        "--model_params_dict_filepath",
        type=str,
        required=False,
        help="Path of the .json file containing parameters of the model to override default "
             "values."
    )
    parser.add_argument(
        "--data_mapping_filepath",
        type=str,
        required=True,
        help="Path of the .json file containing a mapping between column names in the .csv file "
             "containing evidence to the model and config bundle parameters required for "
             "inference."
    )
    parser.add_argument(
        "--do_prior",
        type=int,
        required=False,
        default=1,
        help="Whether to perform prior predictive check. Use the value 0 to deactivate.",
    )
    parser.add_argument(
        "--do_posterior",
        type=int,
        required=False,
        default=1,
        help="Whether to perform posterior inference. Use the value 0 to deactivate.",
    )
    parser.add_argument(
        "--seed",
        type=int,
        required=False,
        default=DEFAULT_SEED,
        help="Random seed to use during inference.",
    )
    parser.add_argument(
        "--burn_in",
        type=int,
        required=False,
        default=DEFAULT_BURN_IN,
        help="Number of samples to discard per chain during posterior inference.",
    )
    parser.add_argument(
        "--num_samples",
        type=int,
        required=False,
        default=DEFAULT_NUM_SAMPLES,
        help="Number of samples to keep per chain during posterior inference.",
    )
    parser.add_argument(
        "--num_chains",
        type=int,
        required=False,
        default=DEFAULT_NUM_CHAINS,
        help="Number of chains to use during posterior inference.",
    )
    parser.add_argument(
        "--num_jobs",
        type=int,
        required=False,
        default=DEFAULT_NUM_JOBS,
        help="Number of jobs to use per inference process. The effective number of jobs is "
             "min(num_jobs, num_chains).",
    )
    parser.add_argument(
        "--nuts_init_method",
        type=str,
        required=False,
        default=DEFAULT_NUTS_INIT_METHOD,
        help="NUTS initialization method.",
    )
    parser.add_argument(
        "--target_accept",
        type=float,
        required=False,
        default=DEFAULT_TARGET_ACCEPT,
        help="Target acceptance probability used to control step size and reduce "
             "divergences during inference.",
    )

    args = parser.parse_args()

    infer(out_dir=args.out_dir,
          experiment_ids=args.experiment_ids.split(","),
          evidence_filepath=args.evidence_filepath,
          model_params_dict_filepath=args.model_params_dict_filepath,
          data_mapping_filepath=args.data_mapping_filepath,
          model_name=args.model_name,
          do_prior=args.do_prior,
          do_posterior=args.do_posterior,
          seed=args.seed,
          burn_in=args.burn_in,
          num_samples=args.num_samples,
          num_chains=args.num_chains,
          num_jobs=args.num_jobs,
          nuts_init_method=args.nuts_init_method,
          target_accept=args.target_accept)
