{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c15938-140a-45db-9bd2-473bf9b7e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee686f7e-2cf1-403d-ac47-ac57da892327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Any, Callable, List, Optional, Tuple\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\") # So we can use the coordination package\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from copy import deepcopy\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from coordination.model.real.vocalic import VocalicModel\n",
    "from coordination.model.config_bundle.vocalic import VocalicConfigBundle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad1a8f6-64f0-48e3-800d-2529f82561de",
   "metadata": {},
   "source": [
    "# Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587d52f1-fe5c-4b7c-9c31-3b3ce91add8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "tex_fonts = {\n",
    "    \"axes.labelsize\": 8,\n",
    "    \"font.size\": 8,\n",
    "    \"legend.fontsize\": 8,\n",
    "    \"xtick.labelsize\": 6,\n",
    "    \"ytick.labelsize\": 6,\n",
    "    \"axes.titlesize\": 8,\n",
    "    \"axes.linewidth\": 1\n",
    "}\n",
    "plt.rcParams.update(tex_fonts)\n",
    "plt.rc('pdf',fonttype = 42)\n",
    "plt.rcParams['text.usetex'] = True\n",
    "\n",
    "# Use \\showthe\\textwidth\n",
    "DOC_WIDTH = int(487.8225 / 2.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdecf72-2655-4371-859a-77ce688eafce",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9b8867-b442-45dd-ba42-27eeea0483d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_best_figure_dimensions(document_width: Union[str, float], scale=1, subplots=(1, 1)):\n",
    "    \"\"\"Set figure dimensions to avoid scaling in LaTeX.\n",
    "    From: https://jwalton.info/Embed-Publication-Matplotlib-Latex/\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    document_width: Union[str, float]\n",
    "            Document textwidth or columnwidth in pts. Predefined strings are also acceptable.\n",
    "    scale: float, optional\n",
    "            Fraction of the width which you wish the figure to occupy\n",
    "    subplots: array-like, optional\n",
    "            The number of rows and columns of subplots.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fig_dim: tuple\n",
    "            Dimensions of figure in inches\n",
    "    \"\"\"\n",
    "    width_pt = document_width\n",
    "\n",
    "    # Width of figure (in pts)\n",
    "    fig_width_pt = width_pt * scale\n",
    "\n",
    "    # Convert from pt to inches\n",
    "    inches_per_pt = 1 / 72.27\n",
    "\n",
    "    # Golden ratio to set aesthetic figure height\n",
    "    # https://disq.us/p/2940ij3\n",
    "    golden_ratio = (5 ** .5 - 1) / 2\n",
    "\n",
    "    # Figure width in inches\n",
    "    fig_width_in = fig_width_pt * inches_per_pt\n",
    "\n",
    "    # Figure height in inches\n",
    "    fig_height_in = fig_width_in * golden_ratio * (subplots[0] / subplots[1])\n",
    "\n",
    "    return fig_width_in, fig_height_in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2ba6e4-779e-4647-865b-2adac64ae9e8",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a70279-8049-49c4-bc64-760b0087dbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "COLORS = [\"#137513\", \"#FF9090\", \"#13B2FF\"]\n",
    "MUSTARD = \"#BE9700\"\n",
    "SUBJECT_NAMES = [\"Bob\", \"Alice\", \"Dan\"]\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 0\n",
    "\n",
    "# Data \n",
    "T=30\n",
    "BURN_IN = 2000\n",
    "NUM_SAMPLES = 2000\n",
    "NUM_CHAINS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08290a49-caa1-4acc-a1c5-14bbbe4acc0b",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0d3011-c916-4bcb-ac16-1566b9d5e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config bundle for a model of vocalics with two features\n",
    "def get_config_bundle(num_time_steps, coordination, constant_coordination, process_noise = 0, observation_noise = 0):\n",
    "    bundle = VocalicConfigBundle()\n",
    "    bundle.num_time_steps_in_coordination_scale = num_time_steps\n",
    "    bundle.num_vocalic_features = 2\n",
    "    bundle.state_space_dimension_size = 2\n",
    "    bundle.state_space_dimension_names = [\"Latent Pitch\", \"Latent Intensity\"]\n",
    "    bundle.vocalic_feature_names = [\"Pitch\", \"Intensity\"]\n",
    "    bundle.sd_a = process_noise\n",
    "    bundle.sd_o = observation_noise\n",
    "    bundle.mean_uc0 = 0.0\n",
    "    bundle.sd_uc = 1.0\n",
    "    bundle.sd_sd_uc = 0.1\n",
    "    bundle.sd_mean_uc0 = 2\n",
    "    bundle.sd_mean_a0 = 3\n",
    "    bundle.match_vocalic_scale = False\n",
    "    bundle.share_mean_a0_across_subjects = False\n",
    "    bundle.share_mean_a0_across_dimensions = False\n",
    "    bundle.weights = [np.array([[1, 0], [0, 5]])] # different scale per feature\n",
    "    bundle.mean_a0 = np.array([[0,0], [25,25], [50,50]])  # Start at different positions and with different speeds\n",
    "    \n",
    "    # Fix coordination samples so we can generate samples for a fixed coordination series.\n",
    "    if constant_coordination:\n",
    "        bundle.constant_coordination = True\n",
    "        bundle.initial_coordination_samples = np.ones((1, num_time_steps)) * coordination\n",
    "    else:\n",
    "        if isinstance(coordination, np.ndarray):\n",
    "            coordination_series = coordination\n",
    "        else:\n",
    "            coordination_series = np.ones(num_time_steps) * coordination\n",
    "        bundle.initial_coordination_samples = coordination_series[None, :]\n",
    "\n",
    "    return bundle\n",
    "\n",
    "\n",
    "model_no_c = VocalicModel(get_config_bundle(T, 0, False, process_noise = 0, observation_noise = 0))\n",
    "model_low_c = VocalicModel(get_config_bundle(T, 0.25, False, process_noise = 0, observation_noise = 0))\n",
    "model_high_c = VocalicModel(get_config_bundle(T, 0.75, False, process_noise = 0, observation_noise = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc173cb6-6921-4b7f-89b0-1c99c752b2a4",
   "metadata": {},
   "source": [
    "# Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85e484d-0c8d-496d-87ac-a3292e61bfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_observations(samples, ax, dim = 0, dim_name = \"Pitch\", **kwargs):\n",
    "    time_map = samples.component_group_samples[\"state_space\"].time_steps_in_coordination_scale[0]\n",
    "    subject_indices = samples.component_group_samples[\"speech_vocalics\"].subject_indices[0]\n",
    "    values = samples.component_group_samples[\"speech_vocalics\"].values[0]\n",
    "    for subject in range(3):\n",
    "        ts_idx = np.array([t for t, s in enumerate(subject_indices) if s == subject])\n",
    "        ts = np.array([time_map[t] for t in ts_idx])\n",
    "        ys = values[dim, ts_idx]\n",
    "        ax.plot(ts, ys, label=SUBJECT_NAMES[subject], marker=\"o\", linestyle=\"--\", linewidth=0.5, color=COLORS[subject], **kwargs)   \n",
    "        ax.set_xlabel(\"Time Step\")\n",
    "        ax.set_ylabel(dim_name)\n",
    "\n",
    "# Plot data\n",
    "w, h = calculate_best_figure_dimensions(document_width=DOC_WIDTH, scale=1, subplots=(2,2))  \n",
    "fig, axs = plt.subplots(2, 2, figsize=(w,h*1.5))\n",
    "axs[0,1].sharey(axs[0,0])\n",
    "axs[1,1].sharey(axs[1,0])\n",
    "\n",
    "x_slice = [0, 20]\n",
    "\n",
    "data_no_c = model_no_c.draw_samples()\n",
    "data_low_c = model_low_c.draw_samples()\n",
    "data_high_c = model_high_c.draw_samples()\n",
    "\n",
    "plot_observations(data_no_c, axs[0, 0], markersize=3)\n",
    "plot_observations(data_low_c, axs[0, 1], markersize=3)\n",
    "plot_observations(data_high_c, axs[1, 0], markersize=3)\n",
    "\n",
    "axs[0,0].set_xlim(x_slice)\n",
    "axs[0,1].set_xlim(x_slice)\n",
    "axs[1,0].set_xlim(x_slice)\n",
    "axs[0,1].set_ylabel(\"\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a73df1-f616-4f15-87f4-90196427b62d",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70735a65-2f1a-4a12-a75a-e78c38b35a31",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0d47f4-2e37-475a-a4a7-1d68fbf9512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, burn_in, num_samples, num_chains) -> InferenceData:\n",
    "    # Clone the model so we can create new random variables for it.\n",
    "    model.prepare_for_inference()    \n",
    "    idata = model.fit(burn_in = burn_in, num_samples = num_samples, num_chains = num_chains)\n",
    "\n",
    "    # Plot coordination\n",
    "    w, h = calculate_best_figure_dimensions(document_width=DOC_WIDTH, scale=1)  \n",
    "    fig = plt.figure(figsize=(w,h))\n",
    "    idata.plot_time_series_posterior(ax=fig.gca(), variable_name=\"coordination\", include_bands=True, value_bounds=(0, 1), marker=None)\n",
    "    idata.plot_parameter_posterior()\n",
    "    print(idata.generate_convergence_summary())\n",
    "    return idata\n",
    "\n",
    "def run_inference(model, process_noise=0.01, observation_noise=1.0, burn_in=BURN_IN, num_samples=NUM_SAMPLES, num_chains=NUM_CHAINS, clear_vars=True):\n",
    "    bundle = deepcopy(model.config_bundle)\n",
    "    bundle.sd_a = process_noise\n",
    "    bundle.sd_o = observation_noise\n",
    "    bundle.observation_normalization = \"norm_per_feature\"\n",
    "    # bundle.sd_mean_uc0 = 5.0\n",
    "    # bundle.sd_sd_uc = 0.5\n",
    "    cloned_model = type(model)(bundle)\n",
    "    samples = cloned_model.draw_samples()\n",
    "    update_config_bundle_from_samples(cloned_model.config_bundle, samples)\n",
    "\n",
    "    if clear_vars:\n",
    "        bundle.mean_uc0=None\n",
    "        bundle.sd_uc=None\n",
    "        bundle.mean_a0=None\n",
    "        bundle.sd_a=None\n",
    "        bundle.sd_o=None\n",
    "        # Because features are normalized, we can fix the weights to unitary scale\n",
    "        cloned_model.config_bundle.weights = [np.ones((1,2))]  \n",
    "\n",
    "    idata = train(cloned_model, burn_in, num_samples, num_chains)\n",
    "    return idata, samples    \n",
    "\n",
    "def update_config_bundle_from_samples(config_bundle, samples):\n",
    "    # Metadata\n",
    "    config_bundle.prev_time_diff_subject = samples.component_group_samples[\"state_space\"].prev_time_diff_subject[0]\n",
    "    config_bundle.prev_time_same_subject = samples.component_group_samples[\"state_space\"].prev_time_same_subject[0]\n",
    "    config_bundle.subject_indices = samples.component_group_samples[\"speech_vocalics\"].subject_indices[0]\n",
    "    config_bundle.time_steps_in_coordination_scale = samples.component_group_samples[\"speech_vocalics\"].time_steps_in_coordination_scale[0]\n",
    "\n",
    "    # Data\n",
    "    config_bundle.observed_values = samples.component_group_samples[\"speech_vocalics\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d857c2d-06c3-41f2-87f3-a7d72337e4bc",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d5f865-f602-4e77-bad8-14492c542875",
   "metadata": {},
   "source": [
    "## C = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d69efb-5b31-45dd-adad-9edb81aa7fd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idata_no_c, noisy_data_no_c = run_inference(model_no_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc50f85-1bb0-47bf-9a6d-b3ef218bde15",
   "metadata": {},
   "source": [
    "## C = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9bdafb-845f-4606-88b9-796703dedd12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idata_low_c, noisy_data_low_c = run_inference(model_low_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be0081d-6e88-4f59-bac1-86f9cf95c95c",
   "metadata": {},
   "source": [
    "## C = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd9a7ed-22cc-4453-a048-a6a2ad80f299",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idata_high_c, noisy_data_high_c = run_inference(model_high_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41135a41-c909-4643-8cfe-84c65463a0c1",
   "metadata": {},
   "source": [
    "# Final Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc980e9-bf37-4735-909b-b9f31a4c6db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_formatter(x, pos):\n",
    "    result = x\n",
    "    if result == 0:\n",
    "        return '0'\n",
    "    # If not 0, format as multiples of 1000 (1K, 2K, 3K, etc.)\n",
    "    return f'{result/1000:.1f}K'\n",
    "\n",
    "def save_plot(image_name: str, fig: Any, format: str = \"pdf\"):\n",
    "    fig.savefig(f\"../assets/images/{image_name}.{format}\", format=format, bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e74372-51a5-4a3c-a87a-4f38daf89cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h = calculate_best_figure_dimensions(document_width=DOC_WIDTH, scale=1, subplots=(2,2))  \n",
    "fig, axs = plt.subplots(2, 2, figsize=(w, h*1.5))\n",
    "\n",
    "x_slice = [-1, 30]\n",
    "marker_size = 3\n",
    "\n",
    "axs[0,1].sharey(axs[0,0])\n",
    "# axs[1,1].sharey(axs[1,0])\n",
    "sns.despine(ax=axs[0,0])\n",
    "sns.despine(ax=axs[0,1])\n",
    "sns.despine(ax=axs[1,0])\n",
    "sns.despine(ax=axs[1,1])\n",
    "\n",
    "# Data plots\n",
    "plot_observations(data_no_c, axs[0, 0], markersize=marker_size)\n",
    "axs[0,0].set_ylabel(\"Pitch\")\n",
    "axs[0,0].set_xlabel(\"Time Step\")\n",
    "axs[0,0].set_title(\"Data (c = 0)\")\n",
    "axs[0,0].set_xlim(x_slice)\n",
    "\n",
    "plot_observations(data_low_c, axs[0, 1], markersize=marker_size)\n",
    "axs[0,1].set_ylabel(\"\")\n",
    "axs[0,1].set_xlabel(\"Time Step\")\n",
    "axs[0,1].set_title(\"Data (c = 0.25)\")\n",
    "axs[0,1].set_xlim(x_slice)\n",
    "\n",
    "plot_observations(data_high_c, axs[1, 0], markersize=marker_size)\n",
    "axs[1,0].set_ylabel(\"Pitch\")\n",
    "axs[1,0].set_xlabel(\"Time Step\")\n",
    "axs[1,0].set_title(\"Data (c = 0.75)\")\n",
    "axs[1,0].set_xlim(x_slice)\n",
    "\n",
    "idata_no_c.plot_time_series_posterior(\"coordination\", True, ax=axs[1,1], marker=None, linewidth=1, color=\"tab:red\", label=\"c = 0\")\n",
    "idata_low_c.plot_time_series_posterior(\"coordination\", True, ax=axs[1,1], marker=None, linewidth=1, color=MUSTARD, label=\"c = 0.25\")\n",
    "idata_high_c.plot_time_series_posterior(\"coordination\", True, ax=axs[1,1], marker=None, linewidth=1, color=\"tab:blue\", label=\"c = 0.75\")\n",
    "axs[1,1].set_title(\"Inferred Coordination\")\n",
    "axs[1,1].set_xlabel(\"\")\n",
    "axs[1,1].set_ylabel(\"Coordination\")\n",
    "axs[1,1].set_xlabel(\"Time Step\")\n",
    "axs[1,1].set_ylim([0,1])\n",
    "\n",
    "handles, labels = axs[0,0].get_legend_handles_labels()\n",
    "leg = fig.legend(handles, labels, loc='lower right', ncol=3, bbox_to_anchor=[0.83, -0.13], frameon=False, markerscale=1, columnspacing=1.4, title=\"Speaker\")\n",
    "\n",
    "handles, labels = axs[1,1].get_legend_handles_labels()\n",
    "leg = fig.legend(handles, labels, loc='lower right', ncol=3, bbox_to_anchor=[0.91, -0.25], frameon=False, markerscale=1, columnspacing=1.4, title=\"Coordination\")\n",
    "\n",
    "plt.tight_layout()\n",
    "save_plot(\"results_vocalic_model\", fig, \"png\")\n",
    "save_plot(\"results_vocalic_model\", fig, \"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786ea604-f7b2-4db2-bd2f-6ce444f9cba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coordination",
   "language": "python",
   "name": "coordination"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
