from typing import Any, Dict, List, Optional, Tuple
import os
from datetime import datetime
from dateutil.parser import parse
import json
import pickle

from tqdm import tqdm
import numpy as np

from src.components.speech.vocalics_reader import VocalicsReader
from src.config.component_config_bundle import VocalicsConfig


class Utterance:
    def __init__(self, participant_id: str, text: str, start: datetime, end: datetime):
        self.participant_id = participant_id
        self.text = text
        self.start = start
        self.end = end

        # This will contain values for different vocalic features within an utterance.
        # The series is a matrix, with as many rows as the number of features and as many
        # columns as the number of vocalic values in the utterance
        self.vocalic_series = np.array([])


class VocalicsComponent:
    def __init__(self, config: VocalicsConfig):
        self.config = config
        self.reader = VocalicsReader(config.database_config.address, config.database_config.port,
                                     config.database_config.database)
        self.utterances_per_subject: Dict[str, List[Utterance]] = {}

    def save(self, out_dir: str):
        with open(f"{out_dir}/vocalics.pkl", "wb") as f:
            pickle.dump(self.utterances_per_subject, f)

    def load(self, vocalics_dir: str):
        vocalics_path = f"{vocalics_dir}/vocalics.pkl"
        if not os.path.exists(vocalics_path):
            raise Exception(f"Could not find the file vocalics.pkl in {vocalics_dir}.")

        with open(vocalics_path, "rb") as f:
            self.utterances_per_subject = pickle.load(f)

    def parse(self, trial_id: str, subject_id_map: Dict[str, str], asr_messages: List[Any],
              time_range: Optional[Tuple[datetime, datetime]] = None, baseline_time: Optional[datetime] = None):
        """
        Parses a list of ASR messages to extract utterances and their corresponding vocalic features.

        Keyword Arguments:
        :param trial_id: The id of the trial in which utterances come from
        :param subject_id_map: Mapping between subject IDs and Names and their respective color. We will use it to map
        participant IDs in the vocalics to their corresponding color.
        :param asr_messages: A list of ASR messages extracted from a .metadata file
        :param time_range: The range of time to extract utterances from. ASR messages outside the range are discarded.
        :param baseline_time: It corrects the timestamp of the vocalic features such that the first measurement matches
        the baseline time. When vocalic features are generated by replaying the mission, they receive the timestamp of
        the day when the replay was executed. The baseline_time provides a way for us to adjust the timestamps to make
        them compatible with the timestamps of the day the mission was played.
        """

        asr_messages = sorted(
            asr_messages, key=lambda x: parse(x["header"]["timestamp"])
        )

        pbar = tqdm(total=len(asr_messages), desc="Parsing utterances")
        for asr_message in asr_messages:
            start_timestamp = parse(asr_message["data"]["start_timestamp"])
            end_timestamp = parse(asr_message["data"]["end_timestamp"])

            if time_range is not None:
                if start_timestamp > time_range[1]:
                    # Stop looking for utterances after the upper time limit
                    pbar.n = len(asr_messages)
                    pbar.close()
                    break

                if end_timestamp < time_range[0]:
                    # Ignore utterances before the lower time range.
                    # If an utterance started before but finished after the trial started,
                    # we include the full utterance in the list anyway
                    pbar.update()
                    continue

            subject_id = subject_id_map[asr_message["data"]["participant_id"]]
            text = asr_message["data"]["text"]

            utterance = Utterance(subject_id,
                                  text,
                                  start_timestamp,
                                  end_timestamp)

            if subject_id in self.utterances_per_subject:
                self.utterances_per_subject[subject_id].append(utterance)
            else:
                self.utterances_per_subject[subject_id] = [utterance]

            pbar.update()

        if not self.config.no_vocalics:
            self._read_vocalic_features(trial_id, baseline_time, subject_id_map)

    def _read_vocalic_features(self, trial_id: str, baseline_time: Optional[datetime], subject_id_map: Dict[str, str]):
        """
        Reads vocalic feature values for a series of parsed utterances.
        """

        print("Reading vocalics...")
        vocalics_per_subject = self.reader.read(trial_id, self.config.feature_map, baseline_time, subject_id_map)

        pbar = tqdm(total=len(self.utterances_per_subject), desc="Adding vocalics to utterances")
        for subject_id in self.utterances_per_subject.keys():
            if subject_id not in vocalics_per_subject.keys():
                print(
                    f"[WARN] No vocalic features found for subject {subject_id}.")
                continue

            # Sorted per subject
            vocalic_values, vocalic_timestamps = vocalics_per_subject[subject_id]

            t = 0
            for utterance in self.utterances_per_subject[subject_id]:
                # Find start index of vocalic features that matches the start of an utterance
                while t < vocalic_values.shape[1] and vocalic_timestamps[t] < utterance.start:
                    t += 1

                # Collect vocalic feature values within an utterance
                vocalics_in_utterance = []
                while t < vocalic_values.shape[1] and vocalic_timestamps[t] <= utterance.end:
                    vocalics_in_utterance.append(vocalic_values[:, t, np.newaxis])
                    t += 1

                if len(vocalics_in_utterance) > 0:
                    utterance.vocalic_series = np.concatenate(vocalics_in_utterance, axis=1)
                else:
                    print(
                        "[WARN] No vocalic features detected for utterance between " +
                        f"{utterance.start.isoformat()} and {utterance.end.isoformat()}" +
                        f" for subject {subject_id} in trial {trial_id}. Text: {utterance.text}")

            pbar.update()
